{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:53.889977Z","iopub.status.busy":"2023-04-07T22:06:53.889625Z","iopub.status.idle":"2023-04-07T22:06:53.896601Z","shell.execute_reply":"2023-04-07T22:06:53.895721Z","shell.execute_reply.started":"2023-04-07T22:06:53.889945Z"},"trusted":true},"outputs":[],"source":["# Importing packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["# Fill in the missing code\n","The following script is an implementation of a logistic regression model for a binary classification problem. The dataset contains four numerical input attributes and one output attribute (class label 0 or 1).\n","\n","- Columns **Attribute1**, **Attribute2**, **Attribute3**, and **Attribute4** are inputs.\n","- Column **OutputClass** is output.\n","\n","The missing pieces of code are indicated like so: `#write your code here#`.\n","\n","The functions corresponding to said missing code are stated in the description above each function.\n","\n","To complete the task, replace `#write your code here#` with the your own code. Also, ensure that the program executes without any warnings or errors."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:53.901436Z","iopub.status.busy":"2023-04-07T22:06:53.900302Z","iopub.status.idle":"2023-04-07T22:06:53.919483Z","shell.execute_reply":"2023-04-07T22:06:53.918311Z","shell.execute_reply.started":"2023-04-07T22:06:53.901391Z"},"trusted":true},"outputs":[],"source":["# Reading CSV file as pandas dataframe\n","data = pd.read_csv(\"Full_Data.csv\")\n","\n","data_X = data.values[:, :-1]\n","data_Y = data.values[:, -1].astype(\"int\")\n","\n","# Splitting dataset into training and test set\n","X_full, test_X_full, Y, test_Y = train_test_split(data_X, data_Y, test_size=0.4, shuffle=True, random_state=1)"]},{"cell_type":"markdown","metadata":{},"source":["Try out different input column indexes as input data. For instance, if you choose to use input columns 2 and 3; replace `#write your code here#` with `1, 2` (resp.) in the code block below.\n","\n","Example:\n","* `attr1 = 1`\n","* `attr2 = 2`\n","\n","Note column 1 (i.e., column index = 0) is `RowIdx`, and should not be used as an input.\n","\n","Also note, *column index = column number - 1*."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:53.922269Z","iopub.status.busy":"2023-04-07T22:06:53.92183Z","iopub.status.idle":"2023-04-07T22:06:53.928141Z","shell.execute_reply":"2023-04-07T22:06:53.927086Z","shell.execute_reply.started":"2023-04-07T22:06:53.922228Z"},"trusted":true},"outputs":[],"source":["# Removing all columns other than two\n","attr1 = #write your code here#\n","attr2 = #write your code here#\n","X = X_full[:, (attr1, attr2)]\n","test_X = test_X_full[:, (attr1, attr2)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:53.929986Z","iopub.status.busy":"2023-04-07T22:06:53.929375Z","iopub.status.idle":"2023-04-07T22:06:54.116211Z","shell.execute_reply":"2023-04-07T22:06:54.114772Z","shell.execute_reply.started":"2023-04-07T22:06:53.929953Z"},"trusted":true},"outputs":[],"source":["# Ploting attributes\n","plt.plot(X[:, 0][Y==0], X[:, 1][Y==0], \"o\")\n","plt.plot(X[:, 0][Y==1], X[:, 1][Y==1], \"s\")\n","plt.xlabel(\"Attribute 1\")\n","plt.ylabel(\"Attribute 2\")"]},{"cell_type":"markdown","metadata":{},"source":["The code block below constructs $\\mathbf{X}$. Note that $\\mathbf{X}$ includes $x_0 = 1$ column for the bias (i.e., intercept)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.11877Z","iopub.status.busy":"2023-04-07T22:06:54.118475Z","iopub.status.idle":"2023-04-07T22:06:54.125639Z","shell.execute_reply":"2023-04-07T22:06:54.124381Z","shell.execute_reply.started":"2023-04-07T22:06:54.118741Z"},"trusted":true},"outputs":[],"source":["# Combining input data with fabricated output class\n","X = np.hstack((np.ones((X.shape[0], 1)), X))\n","print(\"The regenerated input data has class labels set to 1:\\n\", X)"]},{"cell_type":"markdown","metadata":{},"source":["## Logistic Regression Model\n","\n","For Logistic Regression, our hypothesis is \n","$$\n","\\hat{Y} = h_w(x) = \\frac{1}{1+e^{-(w^{T}x)}}\n","$$\n","The output range of $\\hat{Y}$ is between 0 and 1."]},{"cell_type":"markdown","metadata":{},"source":["### Sigmoid Function\n","\n","The Sigmoid function squishes all its inputs (i.e., values on x-axis) between 0 and 1.\n","$$\n","\\sigma(z) = \\frac{1}{1+e^{-z}}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.127036Z","iopub.status.busy":"2023-04-07T22:06:54.12679Z","iopub.status.idle":"2023-04-07T22:06:54.137237Z","shell.execute_reply":"2023-04-07T22:06:54.136389Z","shell.execute_reply.started":"2023-04-07T22:06:54.127011Z"},"trusted":true},"outputs":[],"source":["# Defining sigmoid function\n","def sigmoid(z):\n","    # z --> input\n","    # sigmoid_z --> output of sigmoid function\n","    z = z.astype(float)\n","    sigmoid_z = #write your code here#\n","    \n","    return sigmoid_z"]},{"cell_type":"markdown","metadata":{},"source":["The cost function for Logistic Regression for binary classification:\n","$$\n","J(data, w) = \\frac{1}{n}\\sum_{i=1}^{n} L(\\hat{Y}^{(i)},Y^{(i)}) = -\\frac{1}{n}\\sum_{i=1}^{n} [Y^{(i)}log(\\hat{Y}^{(i)}) + (1-Y^{(i)})log(1-\\hat{Y}^{(i)})]\n","$$\n","\n","This loss is also called binary cross entropy error."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.138867Z","iopub.status.busy":"2023-04-07T22:06:54.138541Z","iopub.status.idle":"2023-04-07T22:06:54.149001Z","shell.execute_reply":"2023-04-07T22:06:54.147462Z","shell.execute_reply.started":"2023-04-07T22:06:54.138837Z"},"trusted":true},"outputs":[],"source":["# Defining loss function\n","def loss(Y, y_hat):\n","    # Y --> data\n","    # y_hat --> w\n","    loss = #write your code here#\n","    \n","    return -loss"]},{"cell_type":"markdown","metadata":{},"source":["### Gradient of the loss function\n","\n","Using the Gradient Descent Algorithm, optimal values of the parameters can be calculated like so ($\\eta$ →learning rate), the update rules for parameters are as follows:\n","$$\n","w_{t+1} = w_{t} - \\eta*dw\n","$$\n","Where $dw$ is the partial derivative of loss w.r.t parameter $w$. It looks like:\n","$$\n","dw = \\frac{1}{n} * (\\hat{y}-y).\\textbf{X}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.151014Z","iopub.status.busy":"2023-04-07T22:06:54.150406Z","iopub.status.idle":"2023-04-07T22:06:54.16001Z","shell.execute_reply":"2023-04-07T22:06:54.159189Z","shell.execute_reply.started":"2023-04-07T22:06:54.150984Z"},"trusted":true},"outputs":[],"source":["# Defining gradient function\n","def gradients(X, Y, y_hat):\n","    # X --> input\n","    # Y --> true/target value\n","    # y_hat --> hypothesis/predictions\n","    # n --> number of training examples\n","    \n","    n = X.shape[0]\n","    \n","    # Gradient of loss w.r.t weights\n","    dw = #write your code here#\n","    \n","    return dw"]},{"cell_type":"markdown","metadata":{},"source":["Normalize the data before using/computing gradient. It can accelerate the training process. Make sure you don\"t normalize the \"bias\" term (i.e., first column)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.162527Z","iopub.status.busy":"2023-04-07T22:06:54.162152Z","iopub.status.idle":"2023-04-07T22:06:54.170024Z","shell.execute_reply":"2023-04-07T22:06:54.169288Z","shell.execute_reply.started":"2023-04-07T22:06:54.162498Z"},"trusted":true},"outputs":[],"source":["# Defining data normalization function\n","def normalize(X):\n","    # X --> input\n","    # n --> number of training examples\n","    # d --> number of features \n","    n, d = X.shape\n","    \n","    # Normalizing all the d features of X (except the bias (first) column)\n","    for i in range(d-1):\n","        X[:,i+1] = (X[:,i+1] - X[:,i+1].mean(axis=0))/X[:,i+1].std(axis=0)\n","        \n","    return X"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction\n","\n","Now that the functions to learn the parameters are ready, check if the hypothesis ($\\hat{Y}$) is able to predict the output class $Y=1$ or $Y=0$. Note that the hypothesis is the probability of $Y$ being 1 given $\\textbf{X}$ and is parameterized by $w$.\n","\n","Hence, the prediction function will be so —\n","$$\n","\\hat{Y} = 1 \\to w^{T}\\textbf{X}\n","\\geq 0\n","$$\n","$$\n","\\hat{Y} = 0  \\to w^{T}\\textbf{X} < 0\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.172937Z","iopub.status.busy":"2023-04-07T22:06:54.172638Z","iopub.status.idle":"2023-04-07T22:06:54.180553Z","shell.execute_reply":"2023-04-07T22:06:54.179608Z","shell.execute_reply.started":"2023-04-07T22:06:54.172904Z"},"trusted":true},"outputs":[],"source":["# Defining prediction function\n","def predict(X,w):\n","    # X --> Input.\n","    \n","    # Normalizing the inputs.\n","    X = normalize(X)\n","    \n","    # Calculating prediction/y_hat.\n","    preds = sigmoid(np.dot(X, w))\n","    \n","    # Empty List to store predictions.\n","    pred_class = []\n","    pred_class = [#write your code here#]\n","    \n","    return np.array(pred_class)"]},{"cell_type":"markdown","metadata":{},"source":["The decision boundary will be:\n","$$\n","\\hat{Y} = 0.5 \\quad or \\quad w^{T}\\textbf{X} = 0\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.181876Z","iopub.status.busy":"2023-04-07T22:06:54.181624Z","iopub.status.idle":"2023-04-07T22:06:54.193013Z","shell.execute_reply":"2023-04-07T22:06:54.192267Z","shell.execute_reply.started":"2023-04-07T22:06:54.181852Z"},"trusted":true},"outputs":[],"source":["# Defining function to plot decision boundary\n","def plot_decision_boundary(X,w):\n","    ydisp = -(w[0] + w[1] * X)/w[2]\n","    \n","    fig = plt.figure(figsize=(10, 8))\n","    plt.plot(X[:, 1][Y==0], X[:, 2][Y==0], \"^\")\n","    plt.plot(X[:, 1][Y==1], X[:, 2][Y==1], \"s\")\n","    \n","    plt.xlim([-2, 5])\n","    plt.ylim([-2, 5])\n","    plt.xlabel(\"Attribute 1\")\n","    plt.ylabel(\"Attribute 2\")\n","    plt.title(\"Decision Boundary\")\n","    plt.plot(X, ydisp)"]},{"cell_type":"markdown","metadata":{},"source":["Now that all the required blocks for logistic regression model are ready, encode the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.194634Z","iopub.status.busy":"2023-04-07T22:06:54.193916Z","iopub.status.idle":"2023-04-07T22:06:54.208615Z","shell.execute_reply":"2023-04-07T22:06:54.207732Z","shell.execute_reply.started":"2023-04-07T22:06:54.194607Z"},"trusted":true},"outputs":[],"source":["# Defining training function\n","def train(X, Y, epochs, eta):\n","    # X --> input\n","    # Y --> true/target value\n","    # bs --> batch size\n","    # eta --> learning rate\n","    # n-> number of training examples\n","    # d-> number of features \n","    \n","    n, d = X.shape\n","    \n","    # Initializing weights and bias to zeros\n","    w = np.zeros((d,1))\n","    \n","    # Reshaping Y\n","    Y = Y.reshape(n,1)\n","    \n","    # Normalizing the inputs\n","    X = normalize(X)\n","    \n","    # Empty list to store losses\n","    losses = []\n","    \n","    # Training loop\n","    for epoch in range(epochs):\n","        \n","            # Calculating hypothesis/prediction\n","            y_hat = #write your code here#\n","            \n","            # Getting the gradients of loss w.r.t parameters\n","            dw = #write your code here#\n","            \n","            # Updating the parameters.\n","            w = w- eta*dw\n","            \n","            # Calculating loss and appending it in the list\n","            l = #write your code here#\n","            losses.append(l)\n","        \n","    # Returning weights, losses(List)\n","    return w, losses"]},{"cell_type":"markdown","metadata":{},"source":["Train the model and print the results.\n","\n","Try out different learning rates to improve model performance.\n","\n","Example: `w, l = train(X, Y, epochs=100, eta=0.001)`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.210103Z","iopub.status.busy":"2023-04-07T22:06:54.209681Z","iopub.status.idle":"2023-04-07T22:06:54.247528Z","shell.execute_reply":"2023-04-07T22:06:54.246402Z","shell.execute_reply.started":"2023-04-07T22:06:54.210072Z"},"trusted":true},"outputs":[],"source":["# Training model \n","w, l = train(X, Y, epochs=100, eta=#write your code here#)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.249801Z","iopub.status.busy":"2023-04-07T22:06:54.249161Z","iopub.status.idle":"2023-04-07T22:06:54.385119Z","shell.execute_reply":"2023-04-07T22:06:54.384361Z","shell.execute_reply.started":"2023-04-07T22:06:54.249767Z"},"trusted":true},"outputs":[],"source":["# Plotting loss vs. epoch function\n","plt.plot(l)\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.386721Z","iopub.status.busy":"2023-04-07T22:06:54.386231Z","iopub.status.idle":"2023-04-07T22:06:54.399637Z","shell.execute_reply":"2023-04-07T22:06:54.398841Z","shell.execute_reply.started":"2023-04-07T22:06:54.38669Z"},"trusted":true},"outputs":[],"source":["# Printing training accuracy\n","print(\"The accuracy of model is\",(np.sum(1*(Y==predict(X,w)))/len(Y))*100,\"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.401929Z","iopub.status.busy":"2023-04-07T22:06:54.400977Z","iopub.status.idle":"2023-04-07T22:06:54.990067Z","shell.execute_reply":"2023-04-07T22:06:54.988298Z","shell.execute_reply.started":"2023-04-07T22:06:54.401893Z"},"trusted":true},"outputs":[],"source":["# Plotting the decision boundary\n","plot_decision_boundary(X, w)"]},{"cell_type":"markdown","metadata":{},"source":["Run the test data through the trained model, and print the testing accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:54.991905Z","iopub.status.busy":"2023-04-07T22:06:54.991558Z","iopub.status.idle":"2023-04-07T22:06:55.004643Z","shell.execute_reply":"2023-04-07T22:06:55.003215Z","shell.execute_reply.started":"2023-04-07T22:06:54.991866Z"},"trusted":true},"outputs":[],"source":["# Checking test accuracy\n","test_X = np.hstack((np.ones((test_X.shape[0],1)), test_X))\n","ml_predictions = predict(test_X,w)\n","print(\"The test accuracy of model is\",(np.sum(1*(test_Y==ml_predictions))/len(test_Y))*100,\"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T22:06:55.006436Z","iopub.status.busy":"2023-04-07T22:06:55.006062Z","iopub.status.idle":"2023-04-07T22:06:55.025709Z","shell.execute_reply":"2023-04-07T22:06:55.024531Z","shell.execute_reply.started":"2023-04-07T22:06:55.006407Z"},"trusted":true},"outputs":[],"source":["# Exporting results\n","row_idx = np.concatenate((test_X_full[:, 0].astype(int), X_full[:, 0].astype(int)))\n","opt_cls = np.concatenate((ml_predictions.astype(int), Y.astype(int)))\n","                              \n","output_data = np.concatenate((row_idx.reshape(-1,1), opt_cls.reshape(-1,1)), axis=1)\n","pd.DataFrame(output_data, columns=[\"RowIdx\", \"OutputClass\"]).to_csv(\"Output_Data.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
